{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Author: Roshan Ram\n",
    "# AndrewID: rram\n",
    "\n",
    "import yfinance as yfinance\n",
    "\n",
    "import module_manager\n",
    "module_manager.review()\n",
    "\n",
    "import yfinance as yf # to pull stock data with yf.download(name, yyyy-mm-dd of opening, yyyy-mm-dd of opening)\n",
    "\n",
    "import numpy as np # used for everything lol\n",
    "import pandas as pd # data mainpulation\n",
    "import matplotlib.pyplot as plt # graphing/plotting\n",
    "from keras import *\n",
    "\n",
    "\n",
    "#####\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "####\n",
    "\n",
    "%matplotlib inline \n",
    "#just to make stuff look nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your desired stock. Only alphanumeric characters please.AAPL\n",
      "Enter your desired opening date. (yyyy-mm-dd)2016-08-01\n",
      "Enter your desired closing date. (yyyy-mm-dd)2019-11-01\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock = None\n",
    "while not isinstance(stock, str):\n",
    "    stock = input(\"Enter your desired stock. Only alphanumeric characters please.\")\n",
    "openingInp = input(\"Enter your desired opening date. (yyyy-mm-dd)\") #'2016-01-01'\n",
    "closingInp = input(\"Enter your desired closing date. (yyyy-mm-dd)\") # '2019-08-01'\n",
    "\n",
    "\n",
    "data = yf.download(stock, openingInp, closingInp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test size 656 165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TIME_STEPS = 10 # [rram]\n",
    "BATCH_SIZE = 32 # [rram]\n",
    "\n",
    "\n",
    "#####\n",
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 300,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 60\n",
    "}\n",
    "\n",
    "iter_changes = \"dropout_layers_0.4_0.4\"\n",
    "\n",
    "# INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/inputs\"\n",
    "OUTPUT_PATH = '~/Desktop'\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()\n",
    "###\n",
    "\n",
    "\n",
    "train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "df_train, df_test = train_test_split(data, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train and Test size\", len(df_train), len(df_test))\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    \n",
    "    for i in (range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of time-series i/o (596, 60, 5) (596,)\n",
      "length of time-series i/o (105, 60, 5) (105,)\n"
     ]
    }
   ],
   "source": [
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(20,activation='relu'))\n",
    "lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "optimizer = optimizers.RMSprop(lr=0.00010000)\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 580 samples, validate on 40 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 0.0519 - val_loss: 0.0687\n",
      "Epoch 2/300\n",
      " - 1s - loss: 0.0494 - val_loss: 0.0567\n",
      "Epoch 3/300\n",
      " - 1s - loss: 0.0475 - val_loss: 0.0475\n",
      "Epoch 4/300\n",
      " - 1s - loss: 0.0459 - val_loss: 0.0389\n",
      "Epoch 5/300\n",
      " - 1s - loss: 0.0441 - val_loss: 0.0308\n",
      "Epoch 6/300\n",
      " - 1s - loss: 0.0426 - val_loss: 0.0234\n",
      "Epoch 7/300\n",
      " - 1s - loss: 0.0401 - val_loss: 0.0175\n",
      "Epoch 8/300\n",
      " - 1s - loss: 0.0387 - val_loss: 0.0128\n",
      "Epoch 9/300\n",
      " - 1s - loss: 0.0362 - val_loss: 0.0095\n",
      "Epoch 10/300\n",
      " - 1s - loss: 0.0335 - val_loss: 0.0076\n",
      "Epoch 11/300\n",
      " - 1s - loss: 0.0309 - val_loss: 0.0081\n",
      "Epoch 12/300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'your_log_name' + '.log'), append=True)\n",
    "\n",
    "# epochs = params['epochs']\n",
    "\n",
    "history = model.fit(x_t, y_t, epochs= 300, verbose=2, batch_size=BATCH_SIZE,\n",
    "                    shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                    trim_dataset(y_val, BATCH_SIZE))) #callbacks=[csv_logger])\n",
    "\n",
    "# history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "#                         trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])\n",
    "\n",
    "# convert the predicted value to range of real data\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])\n",
    "\n",
    "# Visualize the training data\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "#plt.show()\n",
    "print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_update_model = True\n",
    "# if model is None or is_update_model:\n",
    "#     from keras import backend as K\n",
    "#     print(\"Building model...\")\n",
    "#     print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "#     model = create_model()\n",
    "    \n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "#                        patience=40, min_delta=0.0001)\n",
    "    \n",
    "#     mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "#                           \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "#                           save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "#     # Not used here. But leaving it here as a reminder for future\n",
    "#     r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "#                                   verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "#     csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "#     history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=BATCH_SIZE,\n",
    "#                         shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "#                         trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, mcp, csv_logger])\n",
    "    \n",
    "#     print(\"saving model...\")\n",
    "#     pickle.dump(model, open(\"lstm_model\", \"wb\"))\n",
    "\n",
    "# # model.evaluate(x_test_t, y_test_t, batch_size=BATCH_SIZE\n",
    "# y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "# y_pred = y_pred.flatten()\n",
    "# y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "# error = mean_squared_error(y_test_t, y_pred)\n",
    "# print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "# print(y_pred[0:15])\n",
    "# print(y_test_t[0:15])\n",
    "\n",
    "# # convert the predicted value to range of real data\n",
    "# y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# # min_max_scaler.inverse_transform(y_pred)\n",
    "# y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# # min_max_scaler.inverse_transform(y_test_t)\n",
    "# print(y_pred_org[0:15])\n",
    "# print(y_test_t_org[0:15])\n",
    "\n",
    "# # Visualize the training data\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# #plt.show()\n",
    "# plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))\n",
    "\n",
    "# # load the saved best model from above\n",
    "# saved_model = load_model(os.path.join(OUTPUT_PATH, 'best_model.h5')) # , \"lstm_best_7-3-19_12AM\",\n",
    "# print(saved_model)\n",
    "\n",
    "# y_pred = saved_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "# y_pred = y_pred.flatten()\n",
    "# y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "# error = mean_squared_error(y_test_t, y_pred)\n",
    "# print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "# print(y_pred[0:15])\n",
    "# print(y_test_t[0:15])\n",
    "# y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "# y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "# print(y_pred_org[0:15])\n",
    "# print(y_test_t_org[0:15])\n",
    "\n",
    "# # Visualize the prediction\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.plot(y_pred_org)\n",
    "# plt.plot(y_test_t_org)\n",
    "# plt.title('Prediction vs Real Stock Price')\n",
    "# plt.ylabel('Price')\n",
    "# plt.xlabel('Days')\n",
    "# plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "# #plt.show()\n",
    "# plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))\n",
    "# print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
