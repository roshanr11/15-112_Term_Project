{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roshan Ram\n",
    "# AndrewID: rram\n",
    "\n",
    "import yfinance as yfinance\n",
    "\n",
    "# import module_manager\n",
    "# module_manager.review()\n",
    "\n",
    "import yfinance as yf # to pull stock data with yf.download(name, yyyy-mm-dd of opening, yyyy-mm-dd of opening)\n",
    "\n",
    "import numpy as np # used for everything lol\n",
    "import pandas as pd # data mainpulation\n",
    "import matplotlib.pyplot as plt # graphing/plotting\n",
    "from keras import *\n",
    "\n",
    "import time, random, copy\n",
    "\n",
    "#####\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "# from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "####\n",
    "\n",
    "# %matplotlib inline \n",
    "#just to make stuff look nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock = None\n",
    "# while not isinstance(stock, str):\n",
    "#     stock = input(\"Enter your desired stock. Only alphanumeric characters please.\")\n",
    "# openingInp = input(\"Enter your desired opening date. (yyyy-mm-dd)\") #'2016-01-01'\n",
    "# closingInp = input(\"Enter your desired closing date. (yyyy-mm-dd)\") # '2019-08-01'\n",
    "\n",
    "\n",
    "# data = yf.download(stock, openingInp, closingInp)\n",
    "\n",
    "\n",
    "# CONSTANTS: ####################################################\n",
    "\n",
    "TIME_STEPS = 10 # [rram]\n",
    "batchSize = 32 # [rram]\n",
    "\n",
    "# CONSTANTS #####################################################\n",
    "\n",
    "def getData_LSTM(app):\n",
    "    stock = None\n",
    "#     while not isinstance(stock, str):\n",
    "    stock = app.getUserInput(\"Enter your desired stock. Only alphanumeric characters please.\")\n",
    "    openingInp = app.getUserInput(\"Enter your desired opening date. (yyyy-mm-dd)\") #'2016-01-01'\n",
    "    closingInp = app.getUserInput(\"Enter your desired closing date. (yyyy-mm-dd)\") # '2019-08-01'\n",
    "\n",
    "\n",
    "    data = yf.download(stock, openingInp, closingInp)\n",
    "    return data, stock \n",
    "\n",
    "# data, stock = getData(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTimesteps(mat, y_col_index):\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "\n",
    "    for i in (range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "def createDataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by batchSize\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def mainFunc(data):\n",
    "\n",
    "    #####\n",
    "    params = {\n",
    "        \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "        \"epochs\": 300,\n",
    "        \"lr\": 0.00010000,\n",
    "        \"time_steps\": 60\n",
    "    }\n",
    "\n",
    "#     iter_changes = \"dropout_layers_0.4_0.4\"\n",
    "\n",
    "    # INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/inputs\"\n",
    "    OUTPUT_PATH = '~/Desktop'\n",
    "    TIME_STEPS = params[\"time_steps\"]\n",
    "    batchSize = params[\"batch_size\"]\n",
    "    stime = time.time()\n",
    "    ###\n",
    "\n",
    "\n",
    "    train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "    df_train, df_test = train_test_split(data, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "    print(\"Train and Test size\", len(df_train), len(df_test))\n",
    "    # scale the feature MinMax, build array\n",
    "    x = df_train.loc[:,train_cols].values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(x)\n",
    "    x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "    \n",
    "    ####\n",
    "        \n",
    "#     x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))   # TESTING 8:07 PM\n",
    "        \n",
    "    x_t, y_t = createTimesteps(x_train, 3)\n",
    "    print('x_t:', x_t, '\\n', 'y_t:', y_t)\n",
    "    x_t = createDataset(x_t, batchSize)\n",
    "    y_t = createDataset(y_t, batchSize)\n",
    "    x_temp, y_temp = createTimesteps(x_test, 3)\n",
    "    x_val, x_test_t = np.split(createDataset(x_temp, batchSize),2)\n",
    "    y_val, y_test_t = np.split(createDataset(y_temp, batchSize),2)\n",
    "    \n",
    "    # def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(batchSize, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.5))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=0.00010000)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    model = lstm_model\n",
    "    \n",
    "    \n",
    "#     csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'your_log_name' + '.log'), append=True)\n",
    "\n",
    "    # epochs = params['epochs']\n",
    "\n",
    "\n",
    "    # NOTE TO SELF CHANGE EPOCHS BACK to 300\n",
    "    history = model.fit(x_t, y_t, epochs= 5, verbose=2, batch_size=batchSize, \n",
    "                        shuffle=False, validation_data=(createDataset(x_val, batchSize), \n",
    "                        createDataset(y_val, batchSize))) #callbacks=[csv_logger])\n",
    "\n",
    "    # history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=batchSize,\n",
    "    #                         shuffle=False, validation_data=(createDataset(x_val, batchSize),\n",
    "    #                         createDataset(y_val, batchSize)), callbacks=[es, mcp, csv_logger])\n",
    "    return history, model, batchSize, x_test_t, y_test_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update: SAVED MODEL!!! IMPORTANT WILL SAVE TIME ON FRONTEND FOR USER [12/4/19]\n",
    "\n",
    "# commented out model saving [12/4/19 7:50 PM ] bc it was throwing some werid error\n",
    "\n",
    "# fileName = 'savedLSTM.h5'\n",
    "# model.save(fileName)\n",
    "# print(f\"Saved model `{fileName}` to disk\")\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plotLoss(history, model, x_test_t, y_test_t):\n",
    "    y_pred = model.predict(createDataset(x_test_t, batchSize), batch_size=batchSize)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_t = createDataset(y_test_t, batchSize)\n",
    "    error = mean_squared_error(y_test_t, y_pred)\n",
    "    print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "    print(y_pred[0:15])\n",
    "    print(y_test_t[0:15])\n",
    "\n",
    "    # convert the predicted value to range of real data\n",
    "    y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "    # min_max_scaler.inverse_transform(y_pred)\n",
    "    y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "    # min_max_scaler.inverse_transform(y_test_t)\n",
    "    print(y_pred_org[0:15])\n",
    "    print(y_test_t_org[0:15])\n",
    "\n",
    "    # Visualize the training data\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = lstm_model.predict(createDataset(x_test_t, batchSize), batch_size=batchSize)\n",
    "# y_pred = y_pred.flatten()\n",
    "# y_test_t = createDataset(y_test_t, batchSize)\n",
    "# error = mean_squared_error(y_test_t, y_pred)\n",
    "# print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "# print(y_pred[0:15])\n",
    "# print(y_test_t[0:15])\n",
    "# y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "# y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "# print(y_pred_org[0:15])\n",
    "# print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plotPrediction(x_test_t, y_test_t, batchSize):\n",
    "    y_pred = lstm_model.predict(createDataset(x_test_t, batchSize), batch_size=batchSize)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_t = createDataset(y_test_t, batchSize)\n",
    "    error = mean_squared_error(y_test_t, y_pred)\n",
    "    print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "    print(y_pred[0:15])\n",
    "    print(y_test_t[0:15])\n",
    "    y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "    y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "    print(y_pred_org[0:15])\n",
    "    print(y_test_t_org[0:15])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(y_pred_org)\n",
    "    plt.plot(y_test_t_org)\n",
    "    plt.title('Prediction vs Real Stock Price')\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Days')\n",
    "    plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "    plt.show()\n",
    "    print(\"program completed \", time.time() - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with real testing data\n",
    "\n",
    "def showErrors(x_test_t, y_test_t, batchSize):\n",
    "    y_pred = model.predict(createDataset(x_test_t, batchSize), batch_size=batchSize) #used to say lstm_model.predict\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_t = createDataset(y_test_t, batchSize)\n",
    "    error = mean_squared_error(y_test_t, y_pred)\n",
    "    print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "    print(y_pred[0:15])\n",
    "    print(y_test_t[0:15])\n",
    "    y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "    y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "    print(y_pred_org[0:15])\n",
    "    print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_update_model = True\n",
    "# if model is None or is_update_model:\n",
    "#     from keras import backend as K\n",
    "#     print(\"Building model...\")\n",
    "#     print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "#     model = create_model()\n",
    "    \n",
    "#     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "#                        patience=40, min_delta=0.0001)\n",
    "    \n",
    "#     mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "#                           \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "#                           save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "#     # Not used here. But leaving it here as a reminder for future\n",
    "#     r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "#                                   verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "#     csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "#     history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=2, batch_size=batchSize,\n",
    "#                         shuffle=False, validation_data=(createDataset(x_val, batchSize),\n",
    "#                         createDataset(y_val, batchSize)), callbacks=[es, mcp, csv_logger])\n",
    "    \n",
    "#     print(\"saving model...\")\n",
    "#     pickle.dump(model, open(\"lstm_model\", \"wb\"))\n",
    "\n",
    "# # model.evaluate(x_test_t, y_test_t, batch_size=batchSize\n",
    "# y_pred = model.predict(createDataset(x_test_t, batchSize), batch_size=batchSize)\n",
    "# y_pred = y_pred.flatten()\n",
    "# y_test_t = createDataset(y_test_t, batchSize)\n",
    "# error = mean_squared_error(y_test_t, y_pred)\n",
    "# print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "# print(y_pred[0:15])\n",
    "# print(y_test_t[0:15])\n",
    "\n",
    "# # convert the predicted value to range of real data\n",
    "# y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# # min_max_scaler.inverse_transform(y_pred)\n",
    "# y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# # min_max_scaler.inverse_transform(y_test_t)\n",
    "# print(y_pred_org[0:15])\n",
    "# print(y_test_t_org[0:15])\n",
    "\n",
    "# # Visualize the training data\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# #plt.show()\n",
    "# plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(batchSize)+\"_\"+time.ctime()+'.png'))\n",
    "\n",
    "# # load the saved best model from above\n",
    "# saved_model = load_model(os.path.join(OUTPUT_PATH, 'best_model.h5')) # , \"lstm_best_7-3-19_12AM\",\n",
    "# print(saved_model)\n",
    "\n",
    "# y_pred = saved_model.predict(createDataset(x_test_t, batchSize), batch_size=batchSize)\n",
    "# y_pred = y_pred.flatten()\n",
    "# y_test_t = createDataset(y_test_t, batchSize)\n",
    "# error = mean_squared_error(y_test_t, y_pred)\n",
    "# print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "# print(y_pred[0:15])\n",
    "# print(y_test_t[0:15])\n",
    "# y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "# y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "# print(y_pred_org[0:15])\n",
    "# print(y_test_t_org[0:15])\n",
    "\n",
    "# # Visualize the prediction\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.plot(y_pred_org)\n",
    "# plt.plot(y_test_t_org)\n",
    "# plt.title('Prediction vs Real Stock Price')\n",
    "# plt.ylabel('Price')\n",
    "# plt.xlabel('Days')\n",
    "# plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "# #plt.show()\n",
    "# plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(batchSize)+\"_\"+time.ctime()+'.png'))\n",
    "# print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
